{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import cnfg\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import time\n",
    "import random\n",
    "#import newspaper\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='nyt_error_log.log',filemode='w', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cnfg.load(\".katie_mongo.cnfg\")\n",
    "mongouser=config['user']\n",
    "mongopwd=config['pwd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###MONGO initialization\n",
    "client = MongoClient(\"mongodb://\"+mongouser+\":\"+mongopwd+\"@localhost/test\") #host='localhost:27017'\n",
    "db = client.test #database\n",
    "table = db.nyttest #\"table\" also called collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(request_page):\n",
    "    '''get date from final page in allowed page requests\n",
    "    and return it in order to build new API query \n",
    "    & access older pages'''\n",
    "    \n",
    "    last_date=datetime.strptime(request_page[0]['pub_date'][:10], '%Y-%m-%d')\n",
    "    prevdate_str = last_date.date().strftime('%Y%m%d')\n",
    "    return prevdate_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(request_page, page_num):\n",
    "    ''' \n",
    "    arguments:\n",
    "    request_page - the 10 item long json file of query results\n",
    "    page_num - the page number of the larger api query\n",
    "    \n",
    "    get_items scrapes the date, url, desk, section name, type of material,  and keywords\n",
    "    for one each query result and will store it in mongo db\n",
    "    '''\n",
    "    for article in request_page:\n",
    "        entry={}\n",
    "        keywords_list=[]\n",
    "        try:\n",
    "            entry['date']=article['pub_date']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            entry['url']=article['web_url']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            entry['source']=article['source']\n",
    "        except:\n",
    "            pass    \n",
    "        try:\n",
    "            entry['news_desk']=article['news_desk']\n",
    "        except:\n",
    "            pass    \n",
    "        try:\n",
    "            entry['section_name']=article['section_name']\n",
    "        except:\n",
    "            pass    \n",
    "        try:\n",
    "            entry['type_of_material']=article['type_of_material']\n",
    "        except:\n",
    "            pass    \n",
    "        try:\n",
    "            for i in range(0, len(article['keywords'])):\n",
    "                keywords_list.append(article['keywords'][i]['value'])\n",
    "            entry['keywords_list']=keywords_list\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # insert into db\n",
    "        try:\n",
    "            table.insert_one(entry)\n",
    "        except:\n",
    "            msg = 'Failed to insert item on page %i into mongodb' % page_num\n",
    "            logging.error(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_api_config = cnfg.load(\"notebooks/.nyt_api_config\")\n",
    "nyt_api_key = nyt_api_config['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY QUERIED: NYT & type_of_material= article,letter,blog\n",
    "#                  NYT & type_of_material= column,editorial\n",
    "\n",
    "startpage=0\n",
    "endpage=150\n",
    "\n",
    "base='https://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
    "q=\"q=abortion\"\n",
    "fq=\"&fq=source:(\\\"The New York Times\\\") AND type_of_material:(\\\"column\\\" \\\"editorial\\\")\"\n",
    "fl=\"&fl=pub_date,headline,news_desk,source,section_name,web_url,keywords,type_of_material\"\n",
    "begin_date=\"&begin_date=19800101\"\n",
    "sort=\"&sort=newest\"\n",
    "facet_field=\"&facet-field=section_name\"\n",
    "facet_filter=\"&facet-filter=true\"\n",
    "a=\"&api-key=\"+nyt_api_key\n",
    "\n",
    "#make server requests for content & url's page by page (0-200), currently 0-150\n",
    "for i in range(startpage,endpage):\n",
    "    page=\"&page=\"+str(i)\n",
    "    querystring= base+q+fq+fl+facet_field+facet_filter+begin_date+page+sort+a\n",
    "    response = requests.get(querystring)\n",
    "    articles=response.json()\n",
    "    \n",
    "    if articles['status']=='OK':\n",
    "        msg=\"Status: Round 0-100, OK Page %i\" % i\n",
    "        print(msg)\n",
    "        get_items(articles['response']['docs'], i)\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        msg = 'Blocked from server at date %d, page %i' % (begin_date,i)\n",
    "        logging.error(msg)\n",
    "        break\n",
    "\n",
    "    #wait 1 seconds before submitting request for next page of results\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# #make server requests for url's page by page - with updated end date (200-400)\n",
    "# end_date=\"&end_date=\"+get_date(articles['response']['docs'])\n",
    "# for i in range(startpage,endpage):\n",
    "#     page=\"&page=\"+str(i)\n",
    "#     querystring= base+q+fq+fl+facet_field+facet_filter+begin_date+end_date+page+sort+a\n",
    "#     response = requests.get(querystring)\n",
    "#     articles=response.json()\n",
    "    \n",
    "#     if articles['status']=='OK':\n",
    "#         msg=\"Status: Round 200-400, OK Page %i\" % i\n",
    "#         print(msg)\n",
    "#         get_items(articles['response']['docs'], i)\n",
    "#         clear_output(wait=True)\n",
    "#     else:\n",
    "#         msg = 'Blocked from server at date %d, page %i' % (begin_date,i)\n",
    "#         logging.error(msg)\n",
    "#         break\n",
    "\n",
    "#     #wait 1 seconds before submitting request for next page of results\n",
    "#     time.sleep(1)\n",
    "    \n",
    "\n",
    "# #make server requests for url's page by page - with updated end date (400-600)\n",
    "# end_date=\"&end_date=\"+get_date(articles['response']['docs'])\n",
    "# for i in range(startpage,endpage):\n",
    "#     page=\"&page=\"+str(i)\n",
    "#     querystring= base+q+fq+fl+facet_field+facet_filter+begin_date+end_date+page+sort+a\n",
    "#     response = requests.get(querystring)\n",
    "#     articles=response.json()\n",
    "    \n",
    "#     if articles['status']=='OK':\n",
    "#         msg=\"Status: Round 400-600, OK Page %i\" % i\n",
    "#         print(msg)\n",
    "#         get_items(articles['response']['docs'], i)\n",
    "#         clear_output(wait=True)\n",
    "#     else:\n",
    "#         msg = 'Blocked from server at date %d, page %i' % (begin_date,i)\n",
    "#         logging.error(msg)\n",
    "#         break\n",
    "\n",
    "#     #wait 1 seconds before submitting request for next page of results\n",
    "#     time.sleep(1)\n",
    "    \n",
    "\n",
    "# #make server requests for url's page by page - with updated end date (600-700)\n",
    "# end_date=\"&end_date=\"+get_date(articles['response']['docs'])\n",
    "# endpage=100\n",
    "# for i in range(startpage,endpage):\n",
    "#     page=\"&page=\"+str(i)\n",
    "#     querystring= base+q+fq+fl+facet_field+facet_filter+begin_date+end_date+page+sort+a\n",
    "#     response = requests.get(querystring)\n",
    "#     articles=response.json()\n",
    "    \n",
    "#     if articles['status']=='OK':\n",
    "#         msg=\"Status: Round 600-700, OK Page %i\" % i\n",
    "#         print(msg)\n",
    "#         get_items(articles['response']['docs'], i)\n",
    "#         clear_output(wait=True)\n",
    "#     else:\n",
    "#         msg = 'Blocked from server at date %d, page %i' % (begin_date,i)\n",
    "#         logging.error(msg)\n",
    "#         break\n",
    "\n",
    "#     #wait 1 seconds before submitting request for next page of results\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-eeec1b8c46e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mbegin_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"&begin_date=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mget_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m199\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"&page=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-233ec5cda750>\u001b[0m in \u001b[0;36mget_date\u001b[0;34m(request_page)\u001b[0m\n\u001b[1;32m      4\u001b[0m     & access older pages'''\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlast_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_page\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pub_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprevdate_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprevdate_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# ALREADY QUERIED: NYT & type_of_material= article,letter,blog\n",
    "#                  NYT & type_of_material= column,editorial\n",
    "# NOW QUERING: NYT & type_of_materials= News\n",
    "\n",
    "startpage=0\n",
    "endpage=200\n",
    "\n",
    "base='https://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
    "q=\"q=abortion\"\n",
    "fq=\"&fq=source:(\\\"The New York Times\\\") AND type_of_material:(\\\"News\\\")\"\n",
    "fl=\"&fl=pub_date,headline,news_desk,source,section_name,web_url,keywords,type_of_material\"\n",
    "begin_date=\"&begin_date=20040924\"\n",
    "sort=\"&sort=oldest\"\n",
    "facet_field=\"&facet-field=section_name\"\n",
    "facet_filter=\"&facet-filter=true\"\n",
    "a=\"&api-key=\"+nyt_api_key\n",
    "\n",
    "#make server requests for content & url's page by page (0-200)\n",
    "for i in range(startpage,endpage):\n",
    "    page=\"&page=\"+str(i)\n",
    "    querystring= base+q+fq+fl+facet_field+facet_filter+begin_date+page+sort+a\n",
    "    response = requests.get(querystring)\n",
    "    articles=response.json()\n",
    "    \n",
    "    if articles['status']=='OK':\n",
    "        msg=\"Status: Round 0-200, OK Page %i\" % i\n",
    "        print(msg)\n",
    "        get_items(articles['response']['docs'], i)\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        msg = 'Blocked from server at date %d, page %i' % (begin_date,i)\n",
    "        print(msg)\n",
    "        break\n",
    "\n",
    "    #wait 1 seconds before submitting request for next page of results\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "clear_output()\n",
    "\n",
    "    \n",
    "while(True):\n",
    "    begin_date=\"&begin_date=\"+get_date(articles['response']['docs'])\n",
    "    for i in range(0,199):\n",
    "        page=\"&page=\"+str(i)\n",
    "        querystring= base+q+fq+fl+facet_field+facet_filter+begin_date+page+sort+a\n",
    "        response = requests.get(querystring)\n",
    "        articles=response.json()\n",
    "\n",
    "        if articles['status']=='OK':\n",
    "            msg=\"Status: begin date \"+begin_date+\", OK Page %i\" % i\n",
    "            print(msg)\n",
    "            get_items(articles['response']['docs'], i)\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            msg = 'Blocked from server at date %d, page %i' % (begin_date,i)\n",
    "            print(msg)\n",
    "            break\n",
    "\n",
    "        #wait 1 seconds before submitting request for next page of results\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLANNING\n",
    "Date\n",
    "url\n",
    "Keywords\n",
    "Text\n",
    "news_desk\n",
    "section_name\n",
    "type_of_material=article,blog,column,letter\n",
    "type_of_material!=video,\n",
    "source:(\"The New York Times\") AND type_of_material:(\"article\" \"blog\" \"letter\" \"column\" \"editorial\")\n",
    "\n",
    "\n",
    "keywords_list=[]\n",
    "date=a['pub_date']\n",
    "url=a['web_url']\n",
    "news_desk=a['news_desk']\n",
    "section_name=a['section_name']\n",
    "type_of_material=a['type_of_material']\n",
    "for i in range(0, len(a['keywords'])):\n",
    "    keywords_list.append(a['keywords'][i]['value'])\n",
    "\n",
    "\n",
    "entry['original_text']=get_text(article['web_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape text from articles in mongo db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    ''' \n",
    "    scrape the significant text from the url specified\n",
    "    '''\n",
    "    text=''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    try:\n",
    "        for element in soup.find_all(class_='css-1i0edl6 e2kc3sl0'):\n",
    "            text=text+' '+element.text\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for element in soup.find_all(class_='story-body-text story-content'):\n",
    "            text=text+' '+element.text\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        bod=soup.find(class_='articleBody')\n",
    "        for element in bod.find_all('p'):\n",
    "            text=text+''+element.text\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for element in soup.find_all(class_='story-body-text'):\n",
    "            text=text+' '+element.text\n",
    "    except:\n",
    "        msg = 'Failed to get scrape text from page '+url\n",
    "        print(msg)\n",
    "        return('')\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_scrape():\n",
    "    ''' \n",
    "    iterate through mongodb documents that do NOT have original text yet, insert scraped text into document\n",
    "    '''\n",
    "    cursor=table.find({'original_text':{'$exists': False}})\n",
    "    for item in cursor:\n",
    "        text=get_text(item['url'])\n",
    "        table.update_one({'_id':item['_id']}, {'$set': {'original_text': text} })\n",
    "        #time.sleep(randint(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.find({'original_text':{'$exists': False}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_scrape2():\n",
    "    ''' \n",
    "    iterate through mongodb documents that failed first round\n",
    "    '''\n",
    "    cursor=table.find({'original_text':''})\n",
    "    \n",
    "    for item in cursor:\n",
    "        if requests.get('http://www.nytimes.com').status_code == 200:\n",
    "            text=get_text(item['url'])\n",
    "            table.update_one({'_id':item['_id']}, {'$set': {'original_text': text} }, upsert=False)\n",
    "            time.sleep(random.randint(1,3))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 248 ms, total: 12.9 s\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%time text_scrape2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.find({'original_text':''}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
